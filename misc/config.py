import os.path as osp
import numpy as np
from easydict import EasyDict as edict

__C = edict()
cfg = __C

# Audio preprocess options
__C.AUDIO = edict()
__C.AUDIO.SNR = 50 #dB, how strong noise should be added to artificially generated audio, not used
__C.AUDIO.DITCHER = 0.0 # add some noise to silent: 0.00001
__C.AUDIO.FBANK_NUM = 80
__C.AUDIO.LOW_F = 12 #20
__C.AUDIO.HIGH_F = 7600 #8000
__C.AUDIO.FRAME_SHIFT = 12.5 #10 #12.5 ms
__C.AUDIO.FRAME_LENGTH = 50 #25 #30 #50 ms

# trm preprocess options
__C.AUDIO.FS = 16000
__C.AUDIO.INPUT_CONTROL_RATE = 250
__C.AUDIO.INPUT_CONTROL_SKIP = int(__C.AUDIO.FRAME_SHIFT * __C.AUDIO.INPUT_CONTROL_RATE)
__C.AUDIO.TVS_START = 26
__C.AUDIO.TVS_DIM = 16
__C.AUDIO.TVS_DIM_NAME = ['microInt', 'glotVol', 'aspVol', 'fricVol', 'fricPos', 'fricCF', 'fricBW', 'r1', 'r2', 'r3', 'r4', 'r5', 'r6', 'r7', 'r8', 'velum']
__C.AUDIO.TVS_R0_DIM = 7
__C.AUDIO.TVS_R0_VALUE = 0.8
__C.AUDIO.TVS_R0_VALUE_NORM = 0.0
__C.AUDIO.TVS_DROP_R0 = True 
__C.AUDIO.TRACK_IDX = [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
__C.AUDIO.TRACK_DIM = 13
__C.AUDIO.TRACK_DIM_NAME = ['Volume', 'Tract Length', 'Glottal pitch', 'Glottal tp', 'Glottal tn min', 'Glottal tn max',\
                        'Breathiness', 'Loss factor', 'Aperture radius', 'Mouth coef', 'Nose coef', 'Throat volume', 'Throat cutoff']
# The following padding variables are not generated by model
__C.AUDIO.TRACK_PAD_BEFORE = [cfg.AUDIO.FS, 250, 1, 0, 0]
__C.AUDIO.TRACK_PAD_AFTER = [1.35, 1.96, 1.91, 1.3, 0.73, 25, 1, 54]
# Used when not train to infer utterance level parameters
# __C.AUDIO.NORMALIZED_TRACK_FAKE = [-1.0, 0.0383333, 0.9931817, 0.26666665, -0.3728, -1.0, -1.0, -0.7875, -0.9875, -0.999995, 0.99998, -1.0, -0.9999666]

# For normalization

# Global path for train/test
__C.GLOBAL = edict()
__C.GLOBAL.MOMENTS_PATH = '../misc/example_path/moments.pkl'
__C.GLOBAL.GNUSPEECH_DIR = '../../gnuspeech_sa/build'

# General Training options
__C.TRAIN = edict()
__C.TRAIN.FLAG = True # True: train; False: test
__C.TRAIN.RESTORE = False # True: restore from checkpoints; False build model with fresh parameters
__C.TRAIN.INPLACE = False # Take effect when restore model, to save new checkpoints whether in original folder or in a new folder
__C.TRAIN.ADAPT = False # Whether do adaptation, to speed up, some summaries will be removed when do adaptation 

__C.TRAIN.PROCESSED_AUDIO_DIR = '../demo' # with train/eval/test subfolders
__C.TRAIN.LOG_DIR = 'log_dir/' # training recordings folder
__C.TRAIN.EXP_NAME = 'experiment_name'

__C.TRAIN.PRETRAINED_MODEL_DIR = r'/mnt/Disk2/linan/Workspace/Code/Python/EMSSL-master/train/log_dir/a2t_model_experiment_name_20240628-00:28:12/' # contains checkpoints to restore and continue training
__C.TRAIN.MODEL_FP = r'MODEL_EPOCH_199_loss=0.0018.pkl' # model checkpoint; check with __C.ITERTRAIN.ITERATE_START_N, should match
__C.TRAIN.START_FPS = 'TRAIN_FPS_EPOCH_199.pkl' # old train data, specify when __C.TRAIN.RESTORE=True, automatically generated during training, along with model checkpoints

__C.TRAIN.GPU_ID = [0]
__C.TRAIN.NUM_LOADER_WORKERS = 0 # number of workers for data_loader, 0 with main stream
__C.TRAIN.NUM_PROCESSES = 20 # multi processes to speed up audio synthesis
__C.TRAIN.BATCH_SIZE = 32  # LJSpeech=180, ARCTIC=300
__C.TRAIN.MAX_EPOCH = 10000 # Not used
__C.TRAIN.LOG_INTERVAL = 10 # How many steps to log loss during train
__C.TRAIN.SAMPLE_PER_EPOCH = 10 # how many epoches to add sample images to tensorboard
__C.TRAIN.SNAPSHOT_INTERVAL = 10 # how many epoches to save checkpoint
__C.TRAIN.LOSS_LAMBDA = 0.001 # 0.001
__C.TRAIN.LR = 5e-4 # 5e-4
__C.TRAIN.LR_DECAY_EPOCH = 30 # 250 epochs to 0.3
__C.TRAIN.LR_DECAY_RATE = 0.5
__C.TRAIN.OLD_SAMPLE_DROP_RATE = 0.33 # ratio of old samples to drop, then total number of samples to train will less than 1/(1-x)

# Iterative_train options
__C.ITERTRAIN = edict()
__C.ITERTRAIN.ITERATE_START_N = 0 # 0 for train from scratch
__C.ITERTRAIN.ITERATE_N = 200 # total iteration
__C.ITERTRAIN.EPOCH_PER_ITERATE = 10

# Evaluation options
__C.EVAL = edict()
__C.EVAL.NUM_LOADER_WORKERS = 0 #if 0 use main stream
__C.EVAL.SAMPLE_PER_EPOCH = 10
__C.EVAL.BATCH_SIZE = 90
__C.EVAL.SAMPLE_NUM = 5

# Model util options, also for testing
__C.UTIL = edict()
__C.UTIL.PRETRAINED_MODEL_DIR = '' # contains checkpoints of trained model, for testing
__C.UTIL.MODEL_FP = ''
__C.UTIL.NUM_LOADER_WORKERS = 1
__C.UTIL.GPU_ID = [1]
__C.UTIL.BATCH_SIZE = 40

def _merge_a_into_b(a, b):
    """Merge config dictionary a into config dictionary b, clobbering the
    options in b whenever they are also specified in a.
    """
    if type(a) is not edict:
        return

    for k, v in a.iteritems():
        # a must specify keys that are in b
        if not b.has_key(k):
            raise KeyError('{} is not a valid config key'.format(k))

        # the types must match, too
        old_type = type(b[k])
        if old_type is not type(v):
            if isinstance(b[k], np.ndarray):
                v = np.array(v, dtype=b[k].dtype)
            else:
                raise ValueError(('Type mismatch ({} vs. {}) '
                                  'for config key: {}').format(type(b[k]),
                                                               type(v), k))

        # recursively merge dicts
        if type(v) is edict:
            try:
                _merge_a_into_b(a[k], b[k])
            except:
                print('Error under config key: {}'.format(k))
                raise
        else:
            b[k] = v


def cfg_from_file(filename):
    """Load a config file and merge it into the default options."""
    import yaml
    with open(filename, 'r') as f:
        yaml_cfg = edict(yaml.load(f))

    _merge_a_into_b(yaml_cfg, __C)
